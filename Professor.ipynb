{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ProfessorAI\n",
    "Ask questions and get answers based on lecture content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "import langchain\n",
    "import pinecone\n",
    "import openai\n",
    "import tiktoken\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "from langchain.vectorstores import Pinecone\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTANT Only run this if you want to set your keys using environment variables\n",
    "OPENAI_API_KEY = os.environ.get(\"OPENAI_API_KEY\")\n",
    "PINECONE_API_KEY = os.environ.get(\"PINECONE_API_KEY\")\n",
    "PINCONE_API_ENV = os.environ.get(\"PINECONE_API_ENV\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "embeddings_engine = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "pinecone.init(\n",
    "    api_key=PINECONE_API_KEY,\n",
    "    environment=PINCONE_API_ENV\n",
    ")\n",
    "index_name = \"diskstrukt2023\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#db = Pinecone.from_existing_index(index_name=index_name, embedding=embeddings_engine)\n",
    "db = Chroma(persist_directory=\"../transcription_db\", embedding_function=embeddings_engine)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select what AI model to use to get your answer here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Valid model names as of making are\n",
    "    #\"gpt-4\": 8192,\n",
    "    #\"gpt-4-0314\": 8192,\n",
    "    #\"gpt-4-0613\": 8192,\n",
    "    #\"gpt-4-32k\": 32768,\n",
    "    #\"gpt-4-32k-0314\": 32768,\n",
    "    #\"gpt-4-32k-0613\": 32768,\n",
    "    #\"gpt-3.5-turbo\": 4096,\n",
    "    #\"gpt-3.5-turbo-0301\": 4096,\n",
    "    #\"gpt-3.5-turbo-0613\": 4096,\n",
    "    #\"gpt-3.5-turbo-16k\": 16385,\n",
    "    #\"gpt-3.5-turbo-16k-0613\": 16385,\n",
    "    #\"text-ada-001\": 2049,\n",
    "    #\"ada\": 2049,\n",
    "    #\"text-babbage-001\": 2040,\n",
    "    #\"babbage\": 2049,\n",
    "    #\"text-curie-001\": 2049,\n",
    "    #\"curie\": 2049,\n",
    "    #\"davinci\": 2049,\n",
    "    #\"text-davinci-003\": 4097,\n",
    "    #\"text-davinci-002\": 4097,\n",
    "    #\"code-davinci-002\": 8001,\n",
    "    #\"code-davinci-001\": 8001,\n",
    "    #\"code-cushman-002\": 2048,\n",
    "    #\"code-cushman-001\": 2048,\n",
    "model = \"gpt-3.5-turbo\"\n",
    "\n",
    "#llm = OpenAI(temperature=0, openai_api_key=OPENAI_API_KEY)\n",
    "llm = ChatOpenAI(temperature=0, openai_api_key=OPENAI_API_KEY, model=model)\n",
    "\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "qa_chain = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=db.as_retriever())\n",
    "conversational_qa_chain = ConversationalRetrievalChain.from_llm(llm=llm, retriever=db.as_retriever(), memory=memory)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set your questions here\n",
    "\n",
    "(Questions should work in any language just fine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Was sind binominalkoeffizienten\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nBinomische Koeffizienten sind mathematische Ausdrücke, die die Anzahl der möglichen Kombinationen von Elementen in einer Gruppe angeben. Sie werden häufig verwendet, um die Wahrscheinlichkeit bestimmter Ereignisse zu berechnen.'"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_chain.run(query=question, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'Wie berechnet man diese?',\n",
       " 'chat_history': [HumanMessage(content='Was sind binominalkoeffizienten', additional_kwargs={}, example=False),\n",
       "  AIMessage(content='\\n\\nBinomische Koeffizienten sind mathematische Ausdrücke, die die Anzahl der möglichen Kombinationen von Elementen in einer Gruppe angeben. Sie werden häufig verwendet, um die Wahrscheinlichkeit bestimmter Ereignisse zu berechnen.', additional_kwargs={}, example=False),\n",
       "  HumanMessage(content='Wie berechnet man diese?', additional_kwargs={}, example=False),\n",
       "  AIMessage(content='\\n\\nDie binomischen Koeffizienten können mit der binomischen Formel berechnet werden. Diese Formel lautet: (n über k) = n! / (k! (n-k)!). Hierbei steht n für die Anzahl der Elemente in der Grundmenge, und k steht für die Anzahl der Elemente, die ausgewählt werden.', additional_kwargs={}, example=False)],\n",
       " 'answer': '\\n\\nDie binomischen Koeffizienten können mit der binomischen Formel berechnet werden. Diese Formel lautet: (n über k) = n! / (k! (n-k)!). Hierbei steht n für die Anzahl der Elemente in der Grundmenge, und k steht für die Anzahl der Elemente, die ausgewählt werden.'}"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversational_qa_chain({\"question\": \"Wie berechnet man diese?\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_32081/1497134276.py:7: GradioDeprecationWarning: `layout` parameter is deprecated, and it has no effect\n",
      "  QA_Interface = gr.Interface(fn=answer_question,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7866\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7866/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "\n",
    "def answer_question(question):\n",
    "    question_result = conversational_qa_chain({\"question\": question})\n",
    "    return question_result[\"answer\"]\n",
    "\n",
    "QA_Interface = gr.Interface(fn=answer_question, \n",
    "    inputs=\"text\", \n",
    "    outputs=\"text\",\n",
    "    layout=\"vertical\"\n",
    "    )\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    chatbot = gr.Chatbot()\n",
    "    msg = gr.Textbox()\n",
    "    clear = gr.ClearButton([msg, chatbot])\n",
    "\n",
    "    def respond(message, chat_history):\n",
    "        bot_message = answer_question(message)\n",
    "        chat_history.append((message, bot_message))\n",
    "        return \"\", chat_history\n",
    "\n",
    "    msg.submit(respond, [msg, chatbot], [msg, chatbot])\n",
    "\n",
    "demo.launch()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
